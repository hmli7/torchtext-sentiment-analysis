{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - try bigram\n",
    " - [x] use pretraining embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(input_path, 'dev_text.txt'), 'r', encoding='utf-8') as f:\n",
    "#     dev_text = f.read().strip().split('\\n')\n",
    "\n",
    "# with open(os.path.join(input_path, 'heldout_text.txt'), 'r', encoding='utf-8') as f:\n",
    "#     heldout_text = f.read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_label_path = os.path.join(input_path,'dev_label.txt')\n",
    "# with open(dev_label_path, 'r', encoding='utf-8') as f:\n",
    "#     dev_y = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_data = pd.DataFrame({'text':dev_text, 'label':dev_y})\n",
    "\n",
    "# dev_data.to_csv(os.path.join(input_path, 'dev_data.tsv'), sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pd.DataFrame({'text':heldout_text})\n",
    "# test_data.to_csv(os.path.join(input_path, 'test_data.tsv'), sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paths\n",
    "import baseline_lstm_model\n",
    "import oh_lstm_model\n",
    "import fasttext_model\n",
    "import config\n",
    "import util\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_packages = [paths, baseline_lstm_model, fasttext_model, oh_lstm_model, util, config]\n",
    "for package in reload_packages:\n",
    "    importlib.reload(package)\n",
    "# importlib.reload(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "import random\n",
    "from torchtext.data import TabularDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])\n",
    "def tokenizer(string): \n",
    "    return [word.text.lower() for word in nlp(clean(string))]\n",
    "\n",
    "def clean(text):\n",
    "    '''remove non alphanumeric character, remove links'''\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text)\n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize = tokenizer, include_lengths = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_datafields = [(\"text\", TEXT), (\"label\", LABEL)]\n",
    "dev_dataset = TabularDataset(\n",
    "               path=os.path.join(paths.input_path, 'dev_data.tsv'),\n",
    "               format='tsv',\n",
    "               skip_header=True,\n",
    "               fields=dev_datafields)\n",
    "\n",
    "test_datafields = [(\"text\", TEXT)]\n",
    "test_dataset = TabularDataset(\n",
    "           path=os.path.join(paths.input_path, 'test_data.tsv'),\n",
    "           format='csv',\n",
    "           skip_header=True,\n",
    "           fields=test_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = dev_dataset.split(split_ratio=0.8, random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_dataset, \n",
    "                 max_size = 40000, \n",
    "                 vectors = \"glove.840B.300d\",\n",
    "                 unk_init = torch.Tensor.normal_ # initialize unk and pad with normal distribution\n",
    "                )\n",
    "LABEL.build_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1117, -0.4966,  0.1631, -0.8817,  0.0539,  0.6684, -0.0597, -0.4675,\n",
       "        -0.2153,  0.8840, -0.7584, -0.3689, -0.3424, -1.4020,  0.3206, -1.0219,\n",
       "         0.7988, -0.0923, -0.7049, -1.6024,  0.2891,  0.4899, -0.3853, -0.7120,\n",
       "        -0.1706, -1.4594,  0.2207,  0.2463, -1.3248,  0.6970, -0.6631,  1.2158,\n",
       "        -1.4949,  0.8810, -1.1786, -0.9340, -0.5675, -0.2772, -2.1834,  0.3668,\n",
       "         0.9380,  0.0078, -0.3139, -1.1567,  1.8409, -1.0174,  1.2192,  0.1601,\n",
       "         1.5985, -0.0469, -1.5270, -2.0143, -1.5173,  0.3877, -1.1849,  0.6897,\n",
       "         1.3232,  1.8169,  0.6808,  0.7244,  0.0323, -1.6593, -1.8773,  0.7372,\n",
       "         0.9257,  0.9247,  0.1825, -0.0737,  0.3147, -1.0369,  0.2100,  0.6144,\n",
       "         0.0628, -0.3297, -1.7970,  0.8728,  0.7670, -0.1138, -0.9428,  0.7540,\n",
       "         0.1407, -0.6937, -0.6159, -0.7295,  0.4308,  0.2862, -0.2481,  0.2040,\n",
       "         0.8519, -1.4102, -0.1071, -0.8018,  0.2771,  2.5599, -1.6952,  0.1885,\n",
       "         0.7388,  1.5903, -0.1947, -0.2415,  1.3204,  1.5997, -1.0792, -0.3396,\n",
       "        -0.6780, -0.1261, -1.6770,  1.2068,  0.5722,  0.0653, -0.0235,  0.8876,\n",
       "         0.9570, -0.5510,  2.6617,  2.1479, -0.8555, -0.7208,  1.3755,  0.9988,\n",
       "         0.1469, -0.9053, -0.3587,  1.6374,  0.6897, -0.5844,  0.9078,  0.4848,\n",
       "        -0.2632, -0.5432, -1.6406,  0.9295,  1.2907,  0.2612, -0.5862, -1.5105,\n",
       "        -2.0155,  0.6964, -0.6676, -0.8424,  0.5289, -0.5447,  0.8097,  1.1226,\n",
       "        -0.6129,  0.2229,  0.4593, -1.7031, -1.2777, -0.7428,  0.9711,  0.3551,\n",
       "        -1.5174, -0.3566,  0.9455, -2.1716, -0.8039, -0.4530, -0.2217, -2.0901,\n",
       "        -0.1771, -0.8981, -0.9834,  0.7564,  0.3474,  0.2856, -0.0495,  0.6225,\n",
       "        -1.9217, -1.3176,  1.2286,  0.7025, -0.0170, -0.2830, -0.6446, -0.0199,\n",
       "         0.1123, -0.6601, -0.6243, -0.7416,  0.3422, -0.2873, -0.6726,  0.8843,\n",
       "         0.7211,  0.2214, -0.2737,  0.8612, -0.0609,  2.1073, -0.9930,  1.4080,\n",
       "        -0.7945,  0.6144, -0.2730,  1.6561,  1.3940,  0.6060,  0.2209, -0.8245,\n",
       "         0.7289, -0.7336,  1.5624, -1.0208, -0.0175, -0.9194, -0.9389, -1.9421,\n",
       "         0.2329, -1.1014, -1.2473, -0.7485, -0.9792,  0.8285, -0.2501,  0.1602,\n",
       "         0.7295, -0.4441,  0.8214, -0.6015,  0.9069,  1.5691, -0.1108, -0.2573,\n",
       "         0.5631,  0.0629, -0.0639,  3.0292,  0.1858, -0.1704,  1.0270,  0.0704,\n",
       "         0.3586, -2.3594, -1.3954,  0.8675, -0.2252,  0.1193, -1.0772,  0.2516,\n",
       "        -0.2196,  0.6062,  0.0473,  0.0124,  0.3775,  0.7332, -0.4915, -1.1049,\n",
       "        -0.0391,  0.0416, -1.0468,  0.0335, -1.0545, -0.1973,  0.8522, -0.4066,\n",
       "        -1.4113,  0.0200, -0.4115, -0.3398, -0.5563, -0.6508, -0.2179,  0.7700,\n",
       "        -0.9989,  1.1106,  0.0311,  1.5696,  1.5389,  0.2984,  0.5561, -0.6868,\n",
       "        -1.1067,  2.2689, -2.2510, -1.0115,  0.3828,  1.2363, -1.0535, -0.9404,\n",
       "         0.8782, -0.8629, -0.9917,  0.8485, -0.3333, -1.2527, -0.9746, -1.5467,\n",
       "         1.0704, -1.0470,  1.3770,  0.3989, -1.0777, -0.3185,  2.2770,  0.3890,\n",
       "        -0.2000, -1.4447,  0.8402, -0.8668])"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors[TEXT.vocab.stoi['<unk>']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_dataset, valid_dataset), \n",
    "    batch_size = config.batch_size,\n",
    "    device = device,\n",
    "    sort = False, # whether sort the whole dataset with sortkey\n",
    "    shuffle = True,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch = True, #sort by length for padding\n",
    "    repeat = False)\n",
    "\n",
    "test_iterator = data.Iterator(\n",
    "    test_dataset,\n",
    "    batch_size = 64,\n",
    "    device = device, \n",
    "    sort = False, \n",
    "    shuffle = False,\n",
    "#     sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch = False, # don't wanna sort in testing set\n",
    "    repeat = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'very', 'well', 'made', 'film']"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 21075)]"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x7f1ba8a3c048>, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = nn.AdaptiveAvgPool2d((1,None))\n",
    "# input = torch.randn(123, 2, 256).permute(1,0,2)\n",
    "# output = m(input).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = baseline_lstm_model.BaselineLstm(vocab_size=len(TEXT.vocab), \n",
    "                                            embed_size=300, \n",
    "                                            hidden_size=32, \n",
    "                                            output_dim=1,\n",
    "                                            nlayers=1,\n",
    "                                            bidirectional=True,\n",
    "                                            lstm_dropout=0,\n",
    "                                            dropout=0.6,\n",
    "                                            pad_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "                                            train_embedding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = oh_lstm_model.OhLstm(vocab_size=len(TEXT.vocab), \n",
    "#                                             embed_size=300, \n",
    "#                                             hidden_size=32, \n",
    "#                                             output_dim=1,\n",
    "#                                             nlayers=1,\n",
    "#                                             bidirectional=True,\n",
    "#                                             lstm_dropout=0,\n",
    "#                                             dropout=0.6,\n",
    "#                                             pad_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "#                                             train_embedding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = fasttext_model.Fasttext(vocab_size=len(TEXT.vocab), \n",
    "#                                             embed_size=300,\n",
    "#                                             output_dim=1,\n",
    "#                                             dropout=0,\n",
    "#                                             pad_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "#                                             train_embedding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineLstm(\n",
       "  (embedding): Embedding(22557, 300, padding_idx=1)\n",
       "  (lstm): LSTM(300, 32, bidirectional=True)\n",
       "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.6)\n",
       ")"
      ]
     },
     "execution_count": 689,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ..., -1.4447,  0.8402, -0.8668],\n",
       "        [ 0.1032, -1.6268,  0.5729,  ...,  0.3180, -0.1626, -0.0417],\n",
       "        [ 0.2720, -0.0620, -0.1884,  ...,  0.1302, -0.1832,  0.1323],\n",
       "        ...,\n",
       "        [ 0.9868, -0.0743,  0.3943,  ..., -0.9680, -0.8778,  0.9756],\n",
       "        [ 0.3286, -0.0345,  0.0263,  ..., -1.5719,  0.0982,  0.7309],\n",
       "        [ 0.1896,  0.1672,  0.3407,  ..., -0.3786, -0.0278, -0.1949]])"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "# load embedding\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "# # init unk token\n",
    "# UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "# model.embedding.weight.data[UNK_IDX] = torch.zeros(200)\n",
    "# model.embedding.weight.data[TEXT.vocab.stoi[TEXT.pad_token]] = torch.zeros(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-03)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "best_epoch, best_vali_loss, starting_epoch = 0, 400, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Epoch     0 \n",
      "Epoch: 0\tBatch: 200\tAvg-Loss: 0.6877\tAvg-Acc: 0.5250 \n",
      "Epoch: 0\tBatch: 400\tAvg-Loss: 0.6886\tAvg-Acc: 0.5425 \n",
      "Epoch: 0\tBatch: 600\tAvg-Loss: 0.6667\tAvg-Acc: 0.5825 \n",
      "Epoch: 0\tBatch: 800\tAvg-Loss: 0.6430\tAvg-Acc: 0.6275 \n",
      "Train Loss: 0.7617\tTrain Acc: 0.5581\tVal Loss: 0.7450\tVal Acc: 0.5875 \n",
      "Epoch time used:  44.034775495529175 s \n",
      "### Epoch     1 \n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 0.6075\tAvg-Acc: 0.6975 \n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 0.6278\tAvg-Acc: 0.6500 \n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 0.6211\tAvg-Acc: 0.6400 \n",
      "Epoch: 1\tBatch: 800\tAvg-Loss: 0.5851\tAvg-Acc: 0.7150 \n",
      "Train Loss: 0.5511\tTrain Acc: 0.7350\tVal Loss: 0.5696\tVal Acc: 0.7200 \n",
      "Epoch time used:  36.316930532455444 s \n",
      "### Epoch     2 \n",
      "Epoch: 2\tBatch: 200\tAvg-Loss: 0.5513\tAvg-Acc: 0.7200 \n",
      "Epoch: 2\tBatch: 400\tAvg-Loss: 0.5223\tAvg-Acc: 0.7400 \n",
      "Epoch: 2\tBatch: 600\tAvg-Loss: 0.5500\tAvg-Acc: 0.7125 \n",
      "Epoch: 2\tBatch: 800\tAvg-Loss: 0.4973\tAvg-Acc: 0.7675 \n",
      "Train Loss: 0.5591\tTrain Acc: 0.7156\tVal Loss: 0.7530\tVal Acc: 0.6475 \n",
      "Epoch time used:  36.267173767089844 s \n",
      "### Epoch     3 \n",
      "Epoch: 3\tBatch: 200\tAvg-Loss: 0.5574\tAvg-Acc: 0.7050 \n",
      "Epoch: 3\tBatch: 400\tAvg-Loss: 0.4921\tAvg-Acc: 0.7700 \n",
      "Epoch: 3\tBatch: 600\tAvg-Loss: 0.5081\tAvg-Acc: 0.7350 \n",
      "Epoch: 3\tBatch: 800\tAvg-Loss: 0.4771\tAvg-Acc: 0.7775 \n",
      "Train Loss: 0.4147\tTrain Acc: 0.8125\tVal Loss: 0.4647\tVal Acc: 0.7675 \n",
      "Epoch time used:  37.48128819465637 s \n",
      "### Epoch     4 \n",
      "Epoch: 4\tBatch: 200\tAvg-Loss: 0.4752\tAvg-Acc: 0.7675 \n",
      "Epoch: 4\tBatch: 400\tAvg-Loss: 0.4496\tAvg-Acc: 0.7850 \n",
      "Epoch: 4\tBatch: 600\tAvg-Loss: 0.4029\tAvg-Acc: 0.8350 \n",
      "Epoch: 4\tBatch: 800\tAvg-Loss: 0.4112\tAvg-Acc: 0.8075 \n",
      "Train Loss: 0.3363\tTrain Acc: 0.8519\tVal Loss: 0.4733\tVal Acc: 0.7900 \n",
      "Epoch time used:  36.50575399398804 s \n",
      "### Epoch     5 \n",
      "Epoch: 5\tBatch: 200\tAvg-Loss: 0.3867\tAvg-Acc: 0.8225 \n",
      "Epoch: 5\tBatch: 400\tAvg-Loss: 0.4051\tAvg-Acc: 0.8125 \n",
      "Epoch: 5\tBatch: 600\tAvg-Loss: 0.3749\tAvg-Acc: 0.8325 \n",
      "Epoch: 5\tBatch: 800\tAvg-Loss: 0.3952\tAvg-Acc: 0.8450 \n",
      "Train Loss: 0.3000\tTrain Acc: 0.8794\tVal Loss: 0.3493\tVal Acc: 0.8325 \n",
      "Epoch time used:  37.0556755065918 s \n",
      "### Epoch     6 \n",
      "Epoch: 6\tBatch: 200\tAvg-Loss: 0.3873\tAvg-Acc: 0.8175 \n",
      "Epoch: 6\tBatch: 400\tAvg-Loss: 0.3593\tAvg-Acc: 0.8650 \n",
      "Epoch: 6\tBatch: 600\tAvg-Loss: 0.3632\tAvg-Acc: 0.8375 \n",
      "Epoch: 6\tBatch: 800\tAvg-Loss: 0.3926\tAvg-Acc: 0.8175 \n",
      "Train Loss: 0.2501\tTrain Acc: 0.9044\tVal Loss: 0.4717\tVal Acc: 0.7725 \n",
      "Epoch time used:  37.0766396522522 s \n",
      "### Epoch     7 \n",
      "Epoch: 7\tBatch: 200\tAvg-Loss: 0.3306\tAvg-Acc: 0.8575 \n",
      "Epoch: 7\tBatch: 400\tAvg-Loss: 0.3316\tAvg-Acc: 0.8450 \n",
      "Epoch: 7\tBatch: 600\tAvg-Loss: 0.3180\tAvg-Acc: 0.8650 \n",
      "Epoch: 7\tBatch: 800\tAvg-Loss: 0.3575\tAvg-Acc: 0.8350 \n",
      "Train Loss: 0.2099\tTrain Acc: 0.9181\tVal Loss: 0.4662\tVal Acc: 0.7800 \n",
      "Epoch time used:  37.297775745391846 s \n",
      "### Epoch     8 \n",
      "Epoch: 8\tBatch: 200\tAvg-Loss: 0.3106\tAvg-Acc: 0.8625 \n",
      "Epoch: 8\tBatch: 400\tAvg-Loss: 0.3182\tAvg-Acc: 0.8775 \n",
      "Epoch: 8\tBatch: 600\tAvg-Loss: 0.3833\tAvg-Acc: 0.8425 \n",
      "Epoch: 8\tBatch: 800\tAvg-Loss: 0.3101\tAvg-Acc: 0.8675 \n",
      "Train Loss: 0.2044\tTrain Acc: 0.9275\tVal Loss: 0.3395\tVal Acc: 0.8525 \n",
      "Epoch time used:  37.42810010910034 s \n",
      "### Epoch     9 \n",
      "Epoch: 9\tBatch: 200\tAvg-Loss: 0.3035\tAvg-Acc: 0.8875 \n",
      "Epoch: 9\tBatch: 400\tAvg-Loss: 0.2926\tAvg-Acc: 0.8650 \n",
      "Epoch: 9\tBatch: 600\tAvg-Loss: 0.3019\tAvg-Acc: 0.8625 \n",
      "Epoch: 9\tBatch: 800\tAvg-Loss: 0.3088\tAvg-Acc: 0.8675 \n",
      "Train Loss: 0.1665\tTrain Acc: 0.9413\tVal Loss: 0.4054\tVal Acc: 0.8325 \n",
      "Epoch time used:  37.999223470687866 s \n",
      "### Epoch    10 \n",
      "Epoch: 10\tBatch: 200\tAvg-Loss: 0.2809\tAvg-Acc: 0.8900 \n",
      "Epoch: 10\tBatch: 400\tAvg-Loss: 0.2367\tAvg-Acc: 0.9025 \n",
      "Epoch: 10\tBatch: 600\tAvg-Loss: 0.2479\tAvg-Acc: 0.9025 \n",
      "Epoch: 10\tBatch: 800\tAvg-Loss: 0.2542\tAvg-Acc: 0.8825 \n",
      "Train Loss: 0.1376\tTrain Acc: 0.9506\tVal Loss: 0.3316\tVal Acc: 0.8675 \n",
      "Epoch time used:  37.49714517593384 s \n",
      "### Epoch    11 \n",
      "Epoch: 11\tBatch: 200\tAvg-Loss: 0.2059\tAvg-Acc: 0.9250 \n",
      "Epoch: 11\tBatch: 400\tAvg-Loss: 0.2866\tAvg-Acc: 0.8850 \n",
      "Epoch: 11\tBatch: 600\tAvg-Loss: 0.2070\tAvg-Acc: 0.9150 \n",
      "Epoch: 11\tBatch: 800\tAvg-Loss: 0.2156\tAvg-Acc: 0.9250 \n",
      "Train Loss: 0.1434\tTrain Acc: 0.9456\tVal Loss: 0.3581\tVal Acc: 0.8375 \n",
      "Epoch time used:  36.660014390945435 s \n",
      "### Epoch    12 \n",
      "Epoch: 12\tBatch: 200\tAvg-Loss: 0.2144\tAvg-Acc: 0.9025 \n",
      "Epoch: 12\tBatch: 400\tAvg-Loss: 0.2377\tAvg-Acc: 0.9200 \n",
      "Epoch: 12\tBatch: 600\tAvg-Loss: 0.2290\tAvg-Acc: 0.8900 \n",
      "Epoch: 12\tBatch: 800\tAvg-Loss: 0.2818\tAvg-Acc: 0.8975 \n",
      "Train Loss: 0.1050\tTrain Acc: 0.9688\tVal Loss: 0.3285\tVal Acc: 0.8650 \n",
      "Epoch time used:  35.54521679878235 s \n",
      "### Epoch    13 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-692-e24887775a77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[0mbest_vali_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_vali_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                         \u001b[0mDEVICE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                         start_epoch=starting_epoch)\n\u001b[0m",
      "\u001b[0;32m~/torchtext-sentiment-analysis/src/baseline_lstm_model.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model, optimizer, criterion, train_dataloader, valid_dataloader, best_epoch, best_vali_loss, DEVICE, start_epoch)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;31m# prepare for batch size == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/torchtext-sentiment-analysis/src/baseline_lstm_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, text_lengths, testing)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[sent len, batch size, emb dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mpacked_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mpacked_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_embedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;31m#         #unpack sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#         output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output) # [sent len, batch size, hid dim * num directions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0;32m--> 182\u001b[0;31m                            self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LSTM'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "baseline_lstm_model.run(model, \n",
    "                        optimizer, \n",
    "                        criterion, \n",
    "                        train_iterator, \n",
    "                        valid_iterator, \n",
    "                        best_epoch=best_epoch, \n",
    "                        best_vali_loss=best_vali_loss, \n",
    "                        DEVICE=device, \n",
    "                        start_epoch=starting_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint '../outputs/baseline_lstm_4.pth.tar'\n",
      "=> loaded checkpoint '../outputs/baseline_lstm_4.pth.tar' (epoch 4)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for epoch in [4]:\n",
    "    # checkpoint = torch.load(\"checkpoint.pt\")\n",
    "    model_prediction = oh_lstm_model.OhLstm(vocab_size=len(TEXT.vocab), \n",
    "                                            embed_size=300, \n",
    "                                            hidden_size=32, \n",
    "                                            output_dim=1,\n",
    "                                            nlayers=1,\n",
    "                                            bidirectional=True,\n",
    "                                            lstm_dropout=0,\n",
    "                                            dropout=0.6,\n",
    "                                            pad_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "                                            train_embedding=False)\n",
    "    # proceeding from old models\n",
    "    model_path = os.path.join(paths.output_path, config.model_prefix+str(epoch)+'.pth.tar')\n",
    "    print(\"=> loading checkpoint '{}'\".format(model_path))\n",
    "    checkpoint = torch.load(model_path)\n",
    "    starting_epoch = checkpoint['epoch']\n",
    "    # best_vali_acc = checkpoint['best_vali_acc']\n",
    "    val_loss: checkpoint['val_loss']\n",
    "    val_acc: checkpoint['val_acc']\n",
    "    model_state_dict = checkpoint['model_state_dict']\n",
    "    model_prediction.load_state_dict(model_state_dict)\n",
    "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    best_vali_loss = checkpoint['best_vali_loss']\n",
    "    best_epoch = checkpoint['best_epoch']\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "          .format(model_path, checkpoint['epoch']))\n",
    "    # del checkpoint, model_state_dict\n",
    "\n",
    "    best_epoch\n",
    "\n",
    "    model_prediction.cuda()\n",
    "\n",
    "    prediction = baseline_lstm_model.predict(model_prediction, test_iterator, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_itos = [LABEL.vocab.itos[int(idx)] for idx in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(paths.output_path, 'heldout_pred_nn_val89.txt'), 'w', encoding='utf-8') as f:\n",
    "    [f.write(prediction_string+'\\n') for prediction_string in prediction_itos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate fn lets you control the return value of each batch\n",
    "# for packed_seqs, you want to return your data sorted by length\n",
    "def collate_lines_for_test(seq_list, lens):\n",
    "    inputs = seq_list.permute(1,0).cpu().numpy()\n",
    "#     lens = [len(seq) for seq in inputs]\n",
    "    # sort by length\n",
    "    seq_order = sorted(range(len(lens)), key=lens.__getitem__, reverse=True)\n",
    "    ordered_inputs = torch.tensor([inputs[i] for i in seq_order]).permute(1,0).cuda()\n",
    "    reverse_order = sorted(range(len(lens)), key=seq_order.__getitem__, reverse=False)\n",
    "    return ordered_inputs,reverse_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_inputs, original_lengths = next(iter(test_iterator)).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   11,    34,     6, ...,     1,     1,     1],\n",
       "       [ 4533,  3995,    91, ..., 13735,   185,     4],\n",
       "       [   51,   325,     6, ...,     1,     1,     1],\n",
       "       ...,\n",
       "       [   24,    20,   627, ...,     1,     1,     1],\n",
       "       [  153,    15,     6, ...,     1,     1,     1],\n",
       "       [ 3184,    81,    11, ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_inputs.permute(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs, reverse_order = collate_lines_for_test(original_inputs, original_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(test_inputs.permute(1,0)[reverse_order].permute(1,0),original_inputs).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

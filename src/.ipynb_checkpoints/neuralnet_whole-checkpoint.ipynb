{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - try bigram\n",
    " - [x] use pretraining embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(input_path, 'dev_text.txt'), 'r', encoding='utf-8') as f:\n",
    "#     dev_text = f.read().strip().split('\\n')\n",
    "\n",
    "# with open(os.path.join(input_path, 'heldout_text.txt'), 'r', encoding='utf-8') as f:\n",
    "#     heldout_text = f.read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_label_path = os.path.join(input_path,'dev_label.txt')\n",
    "# with open(dev_label_path, 'r', encoding='utf-8') as f:\n",
    "#     dev_y = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_data = pd.DataFrame({'text':dev_text, 'label':dev_y})\n",
    "\n",
    "# dev_data.to_csv(os.path.join(input_path, 'dev_data.tsv'), sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pd.DataFrame({'text':heldout_text})\n",
    "# test_data.to_csv(os.path.join(input_path, 'test_data.tsv'), sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paths\n",
    "import baseline_lstm_model\n",
    "import oh_lstm_model\n",
    "import fasttext_model\n",
    "import config\n",
    "import util\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_packages = [paths, baseline_lstm_model, fasttext_model, oh_lstm_model, util, config]\n",
    "for package in reload_packages:\n",
    "    importlib.reload(package)\n",
    "# importlib.reload(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "import random\n",
    "from torchtext.data import TabularDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])\n",
    "def tokenizer(string): \n",
    "    return [word.text.lower() for word in nlp(clean(string))]\n",
    "\n",
    "def clean(text):\n",
    "    '''remove non alphanumeric character, remove links'''\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text)\n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize = tokenizer, include_lengths = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_datafields = [(\"text\", TEXT), (\"label\", LABEL)]\n",
    "dev_dataset = TabularDataset(\n",
    "               path=os.path.join(paths.input_path, 'dev_data.tsv'),\n",
    "               format='tsv',\n",
    "               skip_header=True,\n",
    "               fields=dev_datafields)\n",
    "\n",
    "test_datafields = [(\"text\", TEXT)]\n",
    "test_dataset = TabularDataset(\n",
    "           path=os.path.join(paths.input_path, 'test_data.tsv'),\n",
    "           format='csv',\n",
    "           skip_header=True,\n",
    "           fields=test_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset, valid_dataset = dev_dataset.split(split_ratio=0.8, random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(dev_dataset, \n",
    "                 max_size = 40000, \n",
    "                 vectors = \"glove.840B.300d\",\n",
    "                 unk_init = torch.Tensor.normal_ # initialize unk and pad with normal distribution\n",
    "                )\n",
    "LABEL.build_vocab(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.9967e-01,  4.5330e-01, -4.4707e-01, -1.0983e+00, -3.9335e-01,\n",
       "         6.2590e-01, -3.2581e-01, -5.1515e-02,  2.3274e-01,  2.7388e-01,\n",
       "        -1.3581e+00, -8.5221e-02, -5.3399e-01, -1.0420e+00,  6.1497e-01,\n",
       "         1.1353e+00, -1.3872e+00, -2.5153e-01,  4.7529e-01, -7.0218e-01,\n",
       "         1.2106e+00, -6.1420e-01,  2.1234e-01,  1.4733e+00,  6.3766e-01,\n",
       "        -6.7337e-01,  6.1923e-01, -7.5081e-01, -7.8591e-01, -1.1226e-01,\n",
       "        -4.8627e-01, -3.4821e-03, -1.0132e-01, -1.9084e-01,  7.8011e-01,\n",
       "         1.4745e+00,  1.0297e+00, -1.2884e+00, -1.5428e-01,  3.1341e-02,\n",
       "        -5.5076e-01,  1.4309e-01, -1.3872e+00, -6.3138e-02, -4.4608e-01,\n",
       "         8.1426e-01, -4.4729e-02, -5.2960e-01, -4.3132e-01,  9.9671e-01,\n",
       "         7.8646e-01,  2.5492e-01, -5.1689e-01, -2.0364e+00,  1.3325e+00,\n",
       "        -9.4804e-01, -5.2529e-01, -2.5810e-01, -5.6793e-01, -5.3535e-01,\n",
       "        -1.4314e-01,  1.9348e-01, -9.6586e-01, -9.8772e-02, -1.3140e+00,\n",
       "        -4.2755e-01,  5.0416e-01,  2.1112e+00, -3.1460e-01,  3.7596e-01,\n",
       "         2.0207e-01,  4.6499e-02, -1.1899e-01,  1.5696e+00, -5.7835e-01,\n",
       "         1.4790e+00,  6.5676e-01, -8.1103e-01, -1.1006e+00,  9.4337e-01,\n",
       "        -1.4276e-01,  3.0900e-01, -3.6747e-01, -1.3758e+00, -5.1561e-01,\n",
       "        -3.0715e-01,  4.0165e-01, -1.6416e+00,  1.9524e+00,  2.3388e+00,\n",
       "        -3.1872e-01, -1.3894e+00, -3.6197e-01,  1.4418e+00,  3.8283e-01,\n",
       "        -8.8203e-01, -1.8178e+00, -4.9383e-01,  8.8961e-01,  1.8626e+00,\n",
       "        -1.5027e-01, -6.1951e-01,  4.1725e-01,  1.9994e+00,  7.6147e-01,\n",
       "         1.3797e+00, -6.7303e-01,  1.2498e+00,  2.0025e+00, -1.7479e-01,\n",
       "         6.3455e-01,  4.6228e-01,  5.3202e-01,  1.0908e+00,  8.5024e-01,\n",
       "        -1.6571e-01, -7.5575e-01, -1.0601e+00,  1.2862e+00,  4.2554e-01,\n",
       "        -4.4282e-01,  4.6607e-01,  1.6737e-01, -6.5911e-01,  2.0874e+00,\n",
       "        -1.8463e-02, -1.2081e+00, -9.3334e-01, -6.1724e-01,  1.5678e+00,\n",
       "         1.7116e+00, -6.2994e-01, -6.4400e-01,  1.6326e+00,  4.3064e-01,\n",
       "         2.7519e+00, -8.5633e-01, -1.4379e+00, -1.5365e+00, -4.8000e-03,\n",
       "         4.2449e-01,  4.2945e-01, -5.5159e-01, -3.7896e-02, -1.3047e+00,\n",
       "        -4.9051e-01,  3.7015e-01,  1.1809e+00,  1.9712e-02, -5.7551e-02,\n",
       "        -3.8105e-01, -2.8443e+00, -1.0290e+00, -5.4185e-01,  9.3711e-01,\n",
       "         1.4658e-01,  8.5603e-01,  1.8556e-01, -4.8897e-01,  2.5758e-01,\n",
       "         1.0485e-01, -1.3876e+00,  1.8708e-01, -9.1482e-01,  8.4845e-01,\n",
       "        -4.9677e-01,  4.7953e-02,  1.1193e-01,  5.6465e-01, -2.4791e-01,\n",
       "        -7.5645e-01,  4.6660e-01, -3.0706e-01,  1.4340e+00,  3.3215e-01,\n",
       "         3.6213e-01, -4.7588e-01,  1.1275e-01, -7.7910e-02, -8.7730e-01,\n",
       "        -8.2457e-01,  9.2586e-03, -2.2503e-01, -1.6821e-02, -6.9387e-01,\n",
       "        -2.4810e+00, -1.9093e-01,  1.2317e-03, -5.4130e-02, -3.9608e-01,\n",
       "        -5.8315e-01,  7.3539e-01,  2.1531e+00,  3.5946e-01,  1.4459e-01,\n",
       "        -2.7242e-01, -1.8939e+00,  1.2680e+00, -8.8463e-01, -5.9198e-01,\n",
       "        -2.6434e-01, -1.6724e+00, -2.9368e-01, -1.5918e+00,  3.4946e-01,\n",
       "         2.5912e-01, -4.0963e-01, -6.8771e-01, -5.7827e-01, -9.7148e-01,\n",
       "        -3.6588e-02, -3.6454e-01, -4.8337e-01, -5.0652e-01,  8.6331e-02,\n",
       "        -4.8971e-01,  1.4420e+00,  1.3933e+00, -1.2548e+00,  6.3607e-01,\n",
       "        -1.5371e+00,  2.8045e-02,  2.9408e+00,  2.9723e-01,  1.1569e+00,\n",
       "        -3.7600e-01,  2.8423e+00,  1.1522e+00,  1.5618e+00, -9.3699e-01,\n",
       "        -4.4432e-01, -1.9328e+00, -1.5309e+00, -1.2751e-01,  3.8421e-01,\n",
       "         8.1756e-02, -1.0813e+00,  3.9136e-01,  1.0343e-01, -1.3132e+00,\n",
       "        -1.2405e+00,  1.3566e+00,  2.9841e+00, -4.0991e-01,  1.8317e-01,\n",
       "         4.2926e-01, -1.6118e+00, -5.4456e-01, -1.5259e+00,  3.9388e-01,\n",
       "         5.7975e-01,  5.9184e-01,  1.0558e+00,  3.6597e-01, -8.3711e-01,\n",
       "        -3.7468e-01,  8.7487e-01, -1.2763e+00,  1.2900e+00, -6.7641e-01,\n",
       "        -7.6028e-01,  9.3055e-01, -6.3293e-01,  1.3590e+00,  1.0214e+00,\n",
       "         6.8065e-01, -4.7862e-01, -2.8190e-01,  7.4613e-01,  7.9513e-01,\n",
       "         2.3857e-01,  1.8406e+00,  6.8069e-01,  1.3854e+00,  3.6716e-01,\n",
       "        -3.7552e-01,  1.0176e+00,  2.2961e+00, -1.3944e+00, -2.0139e+00,\n",
       "         7.8111e-01,  1.5132e-01, -8.5962e-01, -3.0131e+00,  1.6430e-01,\n",
       "        -1.6838e+00,  1.2936e-01,  5.2411e-01, -6.3378e-01, -1.2869e+00,\n",
       "         1.3558e+00, -2.6067e-02, -3.5950e-01,  2.6389e+00, -1.0202e-01,\n",
       "         2.2828e-02,  1.2567e+00, -2.2015e+00, -1.1077e+00, -1.1893e+00])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors[TEXT.vocab.stoi['<unk>']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "#     (train_dataset, valid_dataset), \n",
    "#     batch_size = config.batch_size,\n",
    "#     device = device,\n",
    "#     sort = False, # whether sort the whole dataset with sortkey\n",
    "#     shuffle = True,\n",
    "#     sort_key = lambda x: len(x.text),\n",
    "#     sort_within_batch = True, #sort by length for padding\n",
    "#     repeat = False)\n",
    "train_iterator = data.Iterator(\n",
    "    dev_dataset,\n",
    "    batch_size = config.batch_size,\n",
    "    device = device, \n",
    "    sort = False, \n",
    "    shuffle = True,\n",
    "    train=True,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch = True, # don't wanna sort in testing set\n",
    "    repeat = False)\n",
    "\n",
    "test_iterator = data.Iterator(\n",
    "    test_dataset,\n",
    "    batch_size = 2,\n",
    "    device = device, \n",
    "    sort = False, \n",
    "    shuffle = False,\n",
    "#     sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch = False, # don't wanna sort in testing set\n",
    "    repeat = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-fb5ca6d7febd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset[0].text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 26213)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x7f7a63511ae8>, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = nn.AdaptiveAvgPool2d((1,None))\n",
    "# input = torch.randn(123, 2, 256).permute(1,0,2)\n",
    "# output = m(input).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = baseline_lstm_model.BaselineLstm(vocab_size=len(TEXT.vocab), \n",
    "#                                             embed_size=300, \n",
    "#                                             hidden_size=32, \n",
    "#                                             output_dim=1,\n",
    "#                                             nlayers=1,\n",
    "#                                             bidirectional=True,\n",
    "#                                             lstm_dropout=0,\n",
    "#                                             dropout=0.6,\n",
    "#                                             pad_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "#                                             train_embedding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = oh_lstm_model.OhLstm(vocab_size=len(TEXT.vocab), \n",
    "                                            embed_size=300, \n",
    "                                            hidden_size=32, \n",
    "                                            output_dim=1,\n",
    "                                            nlayers=1,\n",
    "                                            bidirectional=True,\n",
    "                                            lstm_dropout=0,\n",
    "                                            dropout=0.4,\n",
    "                                            pad_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "                                            train_embedding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = fasttext_model.Fasttext(vocab_size=len(TEXT.vocab), \n",
    "#                                             embed_size=300,\n",
    "#                                             output_dim=1,\n",
    "#                                             dropout=0,\n",
    "#                                             pad_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "#                                             train_embedding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OhLstm(\n",
       "  (embedding): Embedding(25105, 300, padding_idx=1)\n",
       "  (lstm): LSTM(300, 32, bidirectional=True)\n",
       "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (globalpooling): AdaptiveAvgPool2d(output_size=(1, None))\n",
       "  (dropout): Dropout(p=0.4)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4997,  0.4533, -0.4471,  ..., -2.2015, -1.1077, -1.1893],\n",
       "        [ 0.1410, -0.4213,  1.2373,  ..., -1.0298, -0.8967,  0.7318],\n",
       "        [ 0.2720, -0.0620, -0.1884,  ...,  0.1302, -0.1832,  0.1323],\n",
       "        ...,\n",
       "        [-0.5718,  0.6005,  0.2553,  ...,  1.6875,  0.7323,  1.2732],\n",
       "        [ 0.1896,  0.1672,  0.3407,  ..., -0.3786, -0.0278, -0.1949],\n",
       "        [ 0.3195,  0.2435, -0.1777,  ..., -0.0243, -0.6011,  0.2368]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "# load embedding\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "# # init unk token\n",
    "# UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "# model.embedding.weight.data[UNK_IDX] = torch.zeros(200)\n",
    "# model.embedding.weight.data[TEXT.vocab.stoi[TEXT.pad_token]] = torch.zeros(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-03)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "best_epoch, best_vali_loss, starting_epoch = 0, 400, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Epoch     0 \n",
      "Epoch: 0\tBatch: 200\tAvg-Loss: 0.6870\tAvg-Acc: 0.5300 \n",
      "Epoch: 0\tBatch: 400\tAvg-Loss: 0.6618\tAvg-Acc: 0.6075 \n",
      "Epoch: 0\tBatch: 600\tAvg-Loss: 0.5790\tAvg-Acc: 0.7100 \n",
      "Epoch: 0\tBatch: 800\tAvg-Loss: 0.5386\tAvg-Acc: 0.7325 \n",
      "Epoch: 0\tBatch: 1000\tAvg-Loss: 0.5139\tAvg-Acc: 0.7925 \n",
      "Train Loss: 0.4636\tTrain Acc: 0.8035\tVal Loss: 0.4651\tVal Acc: 0.8070 \n",
      "Epoch time used:  216.64469075202942 s \n",
      "### Epoch     1 \n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 0.5952\tAvg-Acc: 0.6875 \n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 0.5346\tAvg-Acc: 0.7400 \n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 0.4823\tAvg-Acc: 0.7625 \n",
      "Epoch: 1\tBatch: 800\tAvg-Loss: 0.4173\tAvg-Acc: 0.8150 \n",
      "Epoch: 1\tBatch: 1000\tAvg-Loss: 0.4784\tAvg-Acc: 0.8025 \n",
      "Train Loss: 0.3965\tTrain Acc: 0.8275\tVal Loss: 0.3969\tVal Acc: 0.8275 \n",
      "Epoch time used:  239.5896201133728 s \n",
      "### Epoch     2 \n",
      "Epoch: 2\tBatch: 200\tAvg-Loss: 0.4625\tAvg-Acc: 0.7950 \n",
      "Epoch: 2\tBatch: 400\tAvg-Loss: 0.4902\tAvg-Acc: 0.8050 \n",
      "Epoch: 2\tBatch: 600\tAvg-Loss: 0.4402\tAvg-Acc: 0.7950 \n",
      "Epoch: 2\tBatch: 800\tAvg-Loss: 0.4035\tAvg-Acc: 0.8325 \n",
      "Epoch: 2\tBatch: 1000\tAvg-Loss: 0.4375\tAvg-Acc: 0.8025 \n",
      "Train Loss: 0.3503\tTrain Acc: 0.8650\tVal Loss: 0.3444\tVal Acc: 0.8650 \n",
      "Epoch time used:  211.45656895637512 s \n",
      "### Epoch     3 \n",
      "Epoch: 3\tBatch: 200\tAvg-Loss: 0.3399\tAvg-Acc: 0.8675 \n",
      "Epoch: 3\tBatch: 400\tAvg-Loss: 0.3988\tAvg-Acc: 0.8300 \n",
      "Epoch: 3\tBatch: 600\tAvg-Loss: 0.3854\tAvg-Acc: 0.8425 \n",
      "Epoch: 3\tBatch: 800\tAvg-Loss: 0.3873\tAvg-Acc: 0.8375 \n",
      "Epoch: 3\tBatch: 1000\tAvg-Loss: 0.4124\tAvg-Acc: 0.8175 \n",
      "Train Loss: 0.3287\tTrain Acc: 0.8825\tVal Loss: 0.3245\tVal Acc: 0.8815 \n",
      "Epoch time used:  204.52193903923035 s \n",
      "### Epoch     4 \n",
      "Epoch: 4\tBatch: 200\tAvg-Loss: 0.3716\tAvg-Acc: 0.8500 \n",
      "Epoch: 4\tBatch: 400\tAvg-Loss: 0.3404\tAvg-Acc: 0.8625 \n",
      "Epoch: 4\tBatch: 600\tAvg-Loss: 0.3765\tAvg-Acc: 0.8425 \n",
      "Epoch: 4\tBatch: 800\tAvg-Loss: 0.3965\tAvg-Acc: 0.8325 \n",
      "Epoch: 4\tBatch: 1000\tAvg-Loss: 0.3804\tAvg-Acc: 0.8475 \n",
      "Train Loss: 0.3039\tTrain Acc: 0.8985\tVal Loss: 0.3075\tVal Acc: 0.8995 \n",
      "Epoch time used:  202.85682106018066 s \n",
      "### Epoch     5 \n",
      "Epoch: 5\tBatch: 200\tAvg-Loss: 0.3427\tAvg-Acc: 0.8700 \n",
      "Epoch: 5\tBatch: 400\tAvg-Loss: 0.3429\tAvg-Acc: 0.8625 \n",
      "Epoch: 5\tBatch: 600\tAvg-Loss: 0.3537\tAvg-Acc: 0.8825 \n",
      "Epoch: 5\tBatch: 800\tAvg-Loss: 0.3794\tAvg-Acc: 0.8350 \n",
      "Epoch: 5\tBatch: 1000\tAvg-Loss: 0.3211\tAvg-Acc: 0.8800 \n",
      "Train Loss: 0.3004\tTrain Acc: 0.8950\tVal Loss: 0.3015\tVal Acc: 0.8950 \n",
      "Epoch time used:  187.1582794189453 s \n",
      "### Epoch     6 \n",
      "Epoch: 6\tBatch: 200\tAvg-Loss: 0.3432\tAvg-Acc: 0.8550 \n",
      "Epoch: 6\tBatch: 400\tAvg-Loss: 0.3024\tAvg-Acc: 0.8825 \n",
      "Epoch: 6\tBatch: 600\tAvg-Loss: 0.2855\tAvg-Acc: 0.8875 \n",
      "Epoch: 6\tBatch: 800\tAvg-Loss: 0.3190\tAvg-Acc: 0.8825 \n",
      "Epoch: 6\tBatch: 1000\tAvg-Loss: 0.3582\tAvg-Acc: 0.8725 \n",
      "Train Loss: 0.2669\tTrain Acc: 0.9050\tVal Loss: 0.2666\tVal Acc: 0.9060 \n",
      "Epoch time used:  183.75095343589783 s \n",
      "### Epoch     7 \n",
      "Epoch: 7\tBatch: 200\tAvg-Loss: 0.3385\tAvg-Acc: 0.8625 \n",
      "Epoch: 7\tBatch: 400\tAvg-Loss: 0.3360\tAvg-Acc: 0.8850 \n",
      "Epoch: 7\tBatch: 600\tAvg-Loss: 0.2651\tAvg-Acc: 0.9200 \n",
      "Epoch: 7\tBatch: 800\tAvg-Loss: 0.2980\tAvg-Acc: 0.8725 \n",
      "Epoch: 7\tBatch: 1000\tAvg-Loss: 0.3359\tAvg-Acc: 0.8750 \n",
      "Train Loss: 0.3055\tTrain Acc: 0.8730\tVal Loss: 0.3041\tVal Acc: 0.8740 \n",
      "Epoch time used:  177.36313772201538 s \n",
      "### Epoch     8 \n",
      "Epoch: 8\tBatch: 200\tAvg-Loss: 0.2400\tAvg-Acc: 0.9250 \n",
      "Epoch: 8\tBatch: 400\tAvg-Loss: 0.3334\tAvg-Acc: 0.8600 \n",
      "Epoch: 8\tBatch: 600\tAvg-Loss: 0.3519\tAvg-Acc: 0.8725 \n",
      "Epoch: 8\tBatch: 800\tAvg-Loss: 0.3395\tAvg-Acc: 0.8675 \n",
      "Epoch: 8\tBatch: 1000\tAvg-Loss: 0.2456\tAvg-Acc: 0.9150 \n",
      "Train Loss: 0.2517\tTrain Acc: 0.9160\tVal Loss: 0.2512\tVal Acc: 0.9160 \n",
      "Epoch time used:  182.45768928527832 s \n",
      "### Epoch     9 \n",
      "Epoch: 9\tBatch: 200\tAvg-Loss: 0.2390\tAvg-Acc: 0.9125 \n",
      "Epoch: 9\tBatch: 400\tAvg-Loss: 0.2861\tAvg-Acc: 0.9000 \n",
      "Epoch: 9\tBatch: 600\tAvg-Loss: 0.3321\tAvg-Acc: 0.8800 \n",
      "Epoch: 9\tBatch: 800\tAvg-Loss: 0.2739\tAvg-Acc: 0.9025 \n",
      "Epoch: 9\tBatch: 1000\tAvg-Loss: 0.2720\tAvg-Acc: 0.8950 \n",
      "Train Loss: 0.2022\tTrain Acc: 0.9350\tVal Loss: 0.2059\tVal Acc: 0.9345 \n",
      "Epoch time used:  186.61375880241394 s \n",
      "### Epoch    10 \n",
      "Epoch: 10\tBatch: 200\tAvg-Loss: 0.2558\tAvg-Acc: 0.8900 \n",
      "Epoch: 10\tBatch: 400\tAvg-Loss: 0.2291\tAvg-Acc: 0.9175 \n",
      "Epoch: 10\tBatch: 600\tAvg-Loss: 0.2573\tAvg-Acc: 0.9025 \n",
      "Epoch: 10\tBatch: 800\tAvg-Loss: 0.2848\tAvg-Acc: 0.9025 \n",
      "Epoch: 10\tBatch: 1000\tAvg-Loss: 0.2382\tAvg-Acc: 0.9050 \n",
      "Train Loss: 0.3281\tTrain Acc: 0.8710\tVal Loss: 0.3501\tVal Acc: 0.8695 \n",
      "Epoch time used:  181.30445194244385 s \n",
      "### Epoch    11 \n",
      "Epoch: 11\tBatch: 200\tAvg-Loss: 0.2410\tAvg-Acc: 0.9125 \n",
      "Epoch: 11\tBatch: 400\tAvg-Loss: 0.2169\tAvg-Acc: 0.9050 \n",
      "Epoch: 11\tBatch: 600\tAvg-Loss: 0.2466\tAvg-Acc: 0.9250 \n",
      "Epoch: 11\tBatch: 800\tAvg-Loss: 0.2880\tAvg-Acc: 0.8950 \n",
      "Epoch: 11\tBatch: 1000\tAvg-Loss: 0.2341\tAvg-Acc: 0.9200 \n",
      "Train Loss: 0.1616\tTrain Acc: 0.9440\tVal Loss: 0.1708\tVal Acc: 0.9450 \n",
      "Epoch time used:  168.4023187160492 s \n",
      "### Epoch    12 \n",
      "Epoch: 12\tBatch: 200\tAvg-Loss: 0.2726\tAvg-Acc: 0.9025 \n",
      "Epoch: 12\tBatch: 400\tAvg-Loss: 0.2381\tAvg-Acc: 0.9100 \n",
      "Epoch: 12\tBatch: 600\tAvg-Loss: 0.2482\tAvg-Acc: 0.9000 \n",
      "Epoch: 12\tBatch: 800\tAvg-Loss: 0.2510\tAvg-Acc: 0.9025 \n",
      "Epoch: 12\tBatch: 1000\tAvg-Loss: 0.2695\tAvg-Acc: 0.9050 \n",
      "Train Loss: 0.1606\tTrain Acc: 0.9550\tVal Loss: 0.1569\tVal Acc: 0.9540 \n",
      "Epoch time used:  163.83074927330017 s \n",
      "### Epoch    13 \n",
      "Epoch: 13\tBatch: 200\tAvg-Loss: 0.2187\tAvg-Acc: 0.9125 \n",
      "Epoch: 13\tBatch: 400\tAvg-Loss: 0.2569\tAvg-Acc: 0.9025 \n",
      "Epoch: 13\tBatch: 600\tAvg-Loss: 0.2491\tAvg-Acc: 0.9100 \n",
      "Epoch: 13\tBatch: 800\tAvg-Loss: 0.2465\tAvg-Acc: 0.9300 \n",
      "Epoch: 13\tBatch: 1000\tAvg-Loss: 0.2024\tAvg-Acc: 0.9250 \n",
      "Train Loss: 0.1275\tTrain Acc: 0.9665\tVal Loss: 0.1258\tVal Acc: 0.9665 \n",
      "Epoch time used:  183.50929975509644 s \n",
      "### Epoch    14 \n",
      "Epoch: 14\tBatch: 200\tAvg-Loss: 0.1826\tAvg-Acc: 0.9450 \n",
      "Epoch: 14\tBatch: 400\tAvg-Loss: 0.1708\tAvg-Acc: 0.9350 \n",
      "Epoch: 14\tBatch: 600\tAvg-Loss: 0.2345\tAvg-Acc: 0.9250 \n",
      "Epoch: 14\tBatch: 800\tAvg-Loss: 0.2006\tAvg-Acc: 0.9325 \n",
      "Epoch: 14\tBatch: 1000\tAvg-Loss: 0.2213\tAvg-Acc: 0.9175 \n",
      "Train Loss: 0.1092\tTrain Acc: 0.9725\tVal Loss: 0.1110\tVal Acc: 0.9730 \n",
      "Epoch time used:  179.85241866111755 s \n",
      "### Epoch    15 \n",
      "Epoch: 15\tBatch: 200\tAvg-Loss: 0.2170\tAvg-Acc: 0.9125 \n",
      "Epoch: 15\tBatch: 400\tAvg-Loss: 0.1845\tAvg-Acc: 0.9525 \n",
      "Epoch: 15\tBatch: 600\tAvg-Loss: 0.1900\tAvg-Acc: 0.9250 \n",
      "Epoch: 15\tBatch: 800\tAvg-Loss: 0.1779\tAvg-Acc: 0.9425 \n",
      "Epoch: 15\tBatch: 1000\tAvg-Loss: 0.1814\tAvg-Acc: 0.9350 \n",
      "Train Loss: 0.0935\tTrain Acc: 0.9780\tVal Loss: 0.0875\tVal Acc: 0.9785 \n",
      "Epoch time used:  174.48371982574463 s \n",
      "### Epoch    16 \n",
      "Epoch: 16\tBatch: 200\tAvg-Loss: 0.2121\tAvg-Acc: 0.9300 \n",
      "Epoch: 16\tBatch: 400\tAvg-Loss: 0.1682\tAvg-Acc: 0.9325 \n",
      "Epoch: 16\tBatch: 600\tAvg-Loss: 0.1949\tAvg-Acc: 0.9300 \n",
      "Epoch: 16\tBatch: 800\tAvg-Loss: 0.1525\tAvg-Acc: 0.9500 \n",
      "Epoch: 16\tBatch: 1000\tAvg-Loss: 0.1654\tAvg-Acc: 0.9550 \n",
      "Train Loss: 0.0839\tTrain Acc: 0.9760\tVal Loss: 0.0852\tVal Acc: 0.9765 \n",
      "Epoch time used:  165.4530053138733 s \n",
      "### Epoch    17 \n",
      "Epoch: 17\tBatch: 200\tAvg-Loss: 0.1845\tAvg-Acc: 0.9375 \n",
      "Epoch: 17\tBatch: 400\tAvg-Loss: 0.1764\tAvg-Acc: 0.9475 \n",
      "Epoch: 17\tBatch: 600\tAvg-Loss: 0.2265\tAvg-Acc: 0.9125 \n",
      "Epoch: 17\tBatch: 800\tAvg-Loss: 0.1349\tAvg-Acc: 0.9575 \n",
      "Epoch: 17\tBatch: 1000\tAvg-Loss: 0.2463\tAvg-Acc: 0.9200 \n",
      "Train Loss: 0.2753\tTrain Acc: 0.8955\tVal Loss: 0.2748\tVal Acc: 0.8935 \n",
      "Epoch time used:  168.18949222564697 s \n",
      "### Epoch    18 \n",
      "Epoch: 18\tBatch: 200\tAvg-Loss: 0.1964\tAvg-Acc: 0.9325 \n",
      "Epoch: 18\tBatch: 400\tAvg-Loss: 0.1452\tAvg-Acc: 0.9575 \n",
      "Epoch: 18\tBatch: 600\tAvg-Loss: 0.1708\tAvg-Acc: 0.9425 \n",
      "Epoch: 18\tBatch: 800\tAvg-Loss: 0.2363\tAvg-Acc: 0.9225 \n",
      "Epoch: 18\tBatch: 1000\tAvg-Loss: 0.1556\tAvg-Acc: 0.9425 \n",
      "Train Loss: 0.0744\tTrain Acc: 0.9815\tVal Loss: 0.0736\tVal Acc: 0.9810 \n",
      "Epoch time used:  158.33103680610657 s \n",
      "### Epoch    19 \n",
      "Epoch: 19\tBatch: 200\tAvg-Loss: 0.1394\tAvg-Acc: 0.9475 \n",
      "Epoch: 19\tBatch: 400\tAvg-Loss: 0.1687\tAvg-Acc: 0.9375 \n",
      "Epoch: 19\tBatch: 600\tAvg-Loss: 0.1734\tAvg-Acc: 0.9400 \n",
      "Epoch: 19\tBatch: 800\tAvg-Loss: 0.1474\tAvg-Acc: 0.9425 \n",
      "Epoch: 19\tBatch: 1000\tAvg-Loss: 0.1538\tAvg-Acc: 0.9500 \n",
      "Train Loss: 0.0666\tTrain Acc: 0.9855\tVal Loss: 0.0656\tVal Acc: 0.9850 \n",
      "Epoch time used:  181.62197709083557 s \n",
      "### Epoch    20 \n",
      "Epoch: 20\tBatch: 200\tAvg-Loss: 0.1258\tAvg-Acc: 0.9550 \n",
      "Epoch: 20\tBatch: 400\tAvg-Loss: 0.1847\tAvg-Acc: 0.9375 \n",
      "Epoch: 20\tBatch: 600\tAvg-Loss: 0.1932\tAvg-Acc: 0.9400 \n",
      "Epoch: 20\tBatch: 800\tAvg-Loss: 0.1365\tAvg-Acc: 0.9625 \n",
      "Epoch: 20\tBatch: 1000\tAvg-Loss: 0.1362\tAvg-Acc: 0.9600 \n",
      "Train Loss: 0.0586\tTrain Acc: 0.9895\tVal Loss: 0.0552\tVal Acc: 0.9905 \n",
      "Epoch time used:  185.36898803710938 s \n",
      "### Epoch    21 \n",
      "Epoch: 21\tBatch: 200\tAvg-Loss: 0.1191\tAvg-Acc: 0.9575 \n",
      "Epoch: 21\tBatch: 400\tAvg-Loss: 0.1262\tAvg-Acc: 0.9550 \n",
      "Epoch: 21\tBatch: 600\tAvg-Loss: 0.1396\tAvg-Acc: 0.9550 \n",
      "Epoch: 21\tBatch: 800\tAvg-Loss: 0.1420\tAvg-Acc: 0.9425 \n",
      "Epoch: 21\tBatch: 1000\tAvg-Loss: 0.1833\tAvg-Acc: 0.9400 \n",
      "Train Loss: 0.0767\tTrain Acc: 0.9885\tVal Loss: 0.0732\tVal Acc: 0.9895 \n",
      "Epoch time used:  187.19093585014343 s \n",
      "### Epoch    22 \n",
      "Epoch: 22\tBatch: 200\tAvg-Loss: 0.1197\tAvg-Acc: 0.9600 \n",
      "Epoch: 22\tBatch: 400\tAvg-Loss: 0.1646\tAvg-Acc: 0.9425 \n",
      "Epoch: 22\tBatch: 600\tAvg-Loss: 0.0974\tAvg-Acc: 0.9775 \n",
      "Epoch: 22\tBatch: 800\tAvg-Loss: 0.1314\tAvg-Acc: 0.9525 \n",
      "Epoch: 22\tBatch: 1000\tAvg-Loss: 0.1318\tAvg-Acc: 0.9575 \n",
      "Train Loss: 0.1206\tTrain Acc: 0.9595\tVal Loss: 0.1223\tVal Acc: 0.9595 \n",
      "Epoch time used:  162.199716091156 s \n",
      "### Epoch    23 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-dbd3e2584da9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                         \u001b[0mDEVICE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                         \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstarting_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                      model_prefix = 'oh_lstm_full_')\n\u001b[0m",
      "\u001b[0;32m~/torchtext-sentiment-analysis/src/oh_lstm_model.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model, optimizer, criterion, train_dataloader, valid_dataloader, best_epoch, best_vali_loss, DEVICE, start_epoch, model_prefix)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mavg_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "oh_lstm_model.run(model, \n",
    "                        optimizer, \n",
    "                        criterion, \n",
    "                        train_iterator, \n",
    "                        train_iterator, \n",
    "                        best_epoch=best_epoch, \n",
    "                        best_vali_loss=best_vali_loss, \n",
    "                        DEVICE=device, \n",
    "                        start_epoch=starting_epoch,\n",
    "                     model_prefix = 'oh_lstm_full_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint '../outputs/oh_lstm_full_8.pth.tar'\n",
      "=> loaded checkpoint '../outputs/oh_lstm_full_8.pth.tar' (epoch 8)\n",
      "0\n",
      "400\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "for epoch in [8]:\n",
    "    # checkpoint = torch.load(\"checkpoint.pt\")\n",
    "    model_prediction = oh_lstm_model.OhLstm(vocab_size=len(TEXT.vocab), \n",
    "                                            embed_size=300, \n",
    "                                            hidden_size=32, \n",
    "                                            output_dim=1,\n",
    "                                            nlayers=1,\n",
    "                                            bidirectional=True,\n",
    "                                            lstm_dropout=0,\n",
    "                                            dropout=0.6,\n",
    "                                            pad_idx=TEXT.vocab.stoi[TEXT.pad_token],\n",
    "                                            train_embedding=False)\n",
    "    # proceeding from old models\n",
    "    model_path = os.path.join(paths.output_path, 'oh_lstm_full_'+str(epoch)+'.pth.tar')\n",
    "    print(\"=> loading checkpoint '{}'\".format(model_path))\n",
    "    checkpoint = torch.load(model_path)\n",
    "    starting_epoch = checkpoint['epoch']\n",
    "    # best_vali_acc = checkpoint['best_vali_acc']\n",
    "    val_loss: checkpoint['val_loss']\n",
    "    val_acc: checkpoint['val_acc']\n",
    "    model_state_dict = checkpoint['model_state_dict']\n",
    "    model_prediction.load_state_dict(model_state_dict)\n",
    "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    best_vali_loss = checkpoint['best_vali_loss']\n",
    "    best_epoch = checkpoint['best_epoch']\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "          .format(model_path, checkpoint['epoch']))\n",
    "    # del checkpoint, model_state_dict\n",
    "\n",
    "    best_epoch\n",
    "\n",
    "    model_prediction.cuda()\n",
    "\n",
    "    prediction = baseline_lstm_model.predict(model_prediction, test_iterator, device)\n",
    "\n",
    "    prediction_itos = [LABEL.vocab.itos[int(idx)] for idx in prediction]\n",
    "\n",
    "    with open(os.path.join(paths.output_path, 'heldout_pred_nn_'+'oh_lstm_full_'+str(epoch)+'.txt'), 'w', encoding='utf-8') as f:\n",
    "        [f.write(prediction_string+'\\n') for prediction_string in prediction_itos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate fn lets you control the return value of each batch\n",
    "# for packed_seqs, you want to return your data sorted by length\n",
    "def collate_lines_for_test(seq_list, lens):\n",
    "    inputs = seq_list.permute(1,0).cpu().numpy()\n",
    "#     lens = [len(seq) for seq in inputs]\n",
    "    # sort by length\n",
    "    seq_order = sorted(range(len(lens)), key=lens.__getitem__, reverse=True)\n",
    "    ordered_inputs = torch.tensor([inputs[i] for i in seq_order]).permute(1,0).cuda()\n",
    "    reverse_order = sorted(range(len(lens)), key=seq_order.__getitem__, reverse=False)\n",
    "    return ordered_inputs,reverse_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_inputs, original_lengths = next(iter(test_iterator)).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   11,    34,     6, ...,     1,     1,     1],\n",
       "       [ 4533,  3995,    91, ..., 13735,   185,     4],\n",
       "       [   51,   325,     6, ...,     1,     1,     1],\n",
       "       ...,\n",
       "       [   24,    20,   627, ...,     1,     1,     1],\n",
       "       [  153,    15,     6, ...,     1,     1,     1],\n",
       "       [ 3184,    81,    11, ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_inputs.permute(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs, reverse_order = collate_lines_for_test(original_inputs, original_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(test_inputs.permute(1,0)[reverse_order].permute(1,0),original_inputs).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

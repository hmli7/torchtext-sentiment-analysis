{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nlp_env\\lib\\site-packages\\ipykernel\\parentpoller.py:116: UserWarning: Parent poll failed.  If the frontend dies,\n",
      "                the kernel may be left running.  Please let us know\n",
      "                about your system (bitness, Python, etc.) at\n",
      "                ipython-dev@scipy.org\n",
      "  ipython-dev@scipy.org\"\"\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import string\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '.'\n",
    "\n",
    "with open(os.path.join(input_path, 'dev_text.txt'), 'r', encoding='utf-8') as f:\n",
    "    dev_text = f.read().split('\\n')\n",
    "\n",
    "with open(os.path.join(input_path, 'heldout_text.txt'), 'r', encoding='utf-8') as f:\n",
    "    heldout_text = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_path = os.path.join(path, 'input', 'dev_text_processed.pkl')\n",
    "dev_label_path = os.path.join(input_path,'dev_label.txt')\n",
    "# test_path = os.path.join(path, 'heldout_text_processed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words refer to spacy.lang.stopwords in sm language model\n",
    "stopwords = {'a',\n",
    " 'about',\n",
    "'across',\n",
    " 'after',\n",
    " 'afterwards',\n",
    " 'again','all',\n",
    " 'almost',\n",
    " 'alone',\n",
    " 'along',\n",
    " 'already',\n",
    " 'also',\n",
    " 'am',\n",
    " 'among',\n",
    " 'amongst',\n",
    " 'amount',\n",
    " 'an',\n",
    " 'and',\n",
    " 'another',\n",
    " 'any',\n",
    " 'anyhow',\n",
    " 'anyone',\n",
    " 'anything',\n",
    " 'anyway',\n",
    " 'anywhere',\n",
    " 'are',\n",
    " 'around',\n",
    " 'as',\n",
    " 'at',\n",
    " 'back',\n",
    " 'be',\n",
    " 'became',\n",
    " 'because',\n",
    " 'become',\n",
    " 'becomes',\n",
    " 'becoming',\n",
    " 'been',\n",
    " 'before',\n",
    " 'beforehand',\n",
    " 'behind',\n",
    " 'being',\n",
    " 'below',\n",
    " 'beside',\n",
    " 'besides',\n",
    " 'between',\n",
    " 'beyond',\n",
    " 'both',\n",
    " 'bottom',\n",
    " 'by',\n",
    " 'ca',\n",
    " 'call',\n",
    " 'could',\n",
    " 'did',\n",
    " 'do',\n",
    " 'does',\n",
    " 'doing',\n",
    " 'done',\n",
    " 'down',\n",
    " 'due',\n",
    " 'during',\n",
    " 'each',\n",
    " 'eight',\n",
    " 'either',\n",
    " 'eleven',\n",
    " 'else',\n",
    " 'elsewhere',\n",
    " 'empty',\n",
    " 'enough',\n",
    " 'even',\n",
    " 'ever',\n",
    " 'every',\n",
    " 'everyone',\n",
    " 'everything',\n",
    " 'everywhere',\n",
    " 'except',\n",
    " 'few',\n",
    " 'fifteen',\n",
    " 'fifty',\n",
    " 'first',\n",
    " 'five',\n",
    " 'for',\n",
    " 'former',\n",
    " 'formerly',\n",
    " 'forty',\n",
    " 'four',\n",
    " 'from',\n",
    " 'front',\n",
    " 'full',\n",
    " 'further',\n",
    " 'get',\n",
    " 'give',\n",
    " 'go',\n",
    " 'had',\n",
    " 'has',\n",
    " 'have',\n",
    " 'he',\n",
    " 'hence',\n",
    " 'her',\n",
    " 'here',\n",
    " 'hereafter',\n",
    " 'hereby',\n",
    " 'herein',\n",
    " 'hereupon',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'him',\n",
    " 'himself',\n",
    " 'his',\n",
    " 'how',\n",
    " 'however',\n",
    " 'hundred',\n",
    " 'i',\n",
    " 'if',\n",
    " 'in',\n",
    " 'into',\n",
    " 'is',\n",
    " 'it',\n",
    " 'its',\n",
    " 'itself',\n",
    " 'just',\n",
    " 'keep',\n",
    " 'last',\n",
    " 'latter',\n",
    " 'latterly',\n",
    " 'least',\n",
    " 'less',\n",
    " 'made',\n",
    " 'make',\n",
    " 'many',\n",
    " 'may',\n",
    " 'me',\n",
    " 'meanwhile',\n",
    " 'might',\n",
    " 'mine',\n",
    " 'more',\n",
    " 'moreover',\n",
    " 'most',\n",
    " 'mostly',\n",
    " 'move',\n",
    " 'much',\n",
    " 'must',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'name',\n",
    " 'namely',\n",
    " 'nevertheless',\n",
    " 'next',\n",
    " 'nine',\n",
    " 'noone',\n",
    " 'nor',\n",
    " 'now',\n",
    " 'nowhere',\n",
    " 'of',\n",
    " 'often',\n",
    " 'on',\n",
    " 'once',\n",
    " 'one',\n",
    " 'only',\n",
    " 'onto',\n",
    " 'or',\n",
    " 'other',\n",
    " 'others',\n",
    " 'otherwise',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'out',\n",
    " 'over',\n",
    " 'own',\n",
    " 'part',\n",
    " 'per',\n",
    " 'perhaps',\n",
    " 'please',\n",
    " 'put',\n",
    " 'quite',\n",
    " 'rather',\n",
    " 're',\n",
    " 'really',\n",
    " 'regarding',\n",
    " 'same',\n",
    " 'say',\n",
    " 'see',\n",
    " 'seem',\n",
    " 'seemed',\n",
    " 'seeming',\n",
    " 'seems',\n",
    " 'serious',\n",
    " 'several',\n",
    " 'she',\n",
    " 'should',\n",
    " 'show',\n",
    " 'side',\n",
    " 'since',\n",
    " 'six',\n",
    " 'sixty',\n",
    " 'so',\n",
    " 'some',\n",
    " 'somehow',\n",
    " 'someone',\n",
    " 'something',\n",
    " 'sometime',\n",
    " 'sometimes',\n",
    " 'somewhere',\n",
    " 'still',\n",
    " 'such',\n",
    " 'take',\n",
    " 'ten',\n",
    " 'than',\n",
    " 'that',\n",
    " 'the',\n",
    " 'their',\n",
    " 'them',\n",
    " 'themselves',\n",
    " 'then',\n",
    " 'thence',\n",
    " 'there',\n",
    " 'thereafter',\n",
    " 'thereby',\n",
    " 'therefore',\n",
    " 'therein',\n",
    " 'thereupon',\n",
    " 'these',\n",
    " 'they',\n",
    " 'third',\n",
    " 'this',\n",
    " 'those',\n",
    " 'though',\n",
    " 'three',\n",
    " 'through',\n",
    " 'throughout',\n",
    " 'thru',\n",
    " 'thus',\n",
    " 'to',\n",
    " 'together',\n",
    " 'too',\n",
    " 'top',\n",
    " 'toward',\n",
    " 'towards',\n",
    " 'twelve',\n",
    " 'twenty',\n",
    " 'two',\n",
    " 'under',\n",
    " 'unless',\n",
    " 'until',\n",
    " 'up',\n",
    " 'upon',\n",
    " 'us',\n",
    " 'used',\n",
    " 'using',\n",
    " 'various',\n",
    " 'very',\n",
    " 'via',\n",
    " 'was',\n",
    " 'we',\n",
    " 'well',\n",
    " 'were',\n",
    " 'what',\n",
    " 'whatever',\n",
    " 'when',\n",
    " 'whence',\n",
    " 'whenever',\n",
    " 'where',\n",
    " 'whereafter',\n",
    " 'whereas',\n",
    " 'whereby',\n",
    " 'wherein',\n",
    " 'whereupon',\n",
    " 'wherever',\n",
    " 'whether',\n",
    " 'which',\n",
    " 'while',\n",
    " 'whither',\n",
    " 'who',\n",
    " 'whoever',\n",
    " 'whole',\n",
    " 'whom',\n",
    " 'whose',\n",
    " 'why',\n",
    " 'will',\n",
    " 'with',\n",
    " 'within',\n",
    " 'would',\n",
    " 'yet',\n",
    " 'you',\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(data_list, deep_clean=False, stop_words=None, add_bigram=False):\n",
    "    '''process the data'''\n",
    "    processed_list = []\n",
    "    for data in data_list:\n",
    "        # tokenize and remove extra spaces\n",
    "        tokens = data.split()\n",
    "        # capitalize and remain only words,remove extra spaces\n",
    "        if stop_words is not None:\n",
    "            remove_list = copy.deepcopy(stop_words)\n",
    "            remove_list.update(set(string.punctuation))\n",
    "            upper_tokens = [clean(token, deep_clean=deep_clean) for token in tokens if token.lower() not in remove_list]\n",
    "        else:\n",
    "            upper_tokens = [clean(token, deep_clean=deep_clean) for token in tokens if token not in string.punctuation]\n",
    "        if add_bigram:\n",
    "            n_grams = set(zip(*[upper_tokens[i:] for i in range(2)]))\n",
    "            upper_tokens.extend(n_grams)\n",
    "        processed_list.append(upper_tokens)\n",
    "    return processed_list\n",
    "    \n",
    "def clean(text, deep_clean=False):\n",
    "    '''remove punctuations within a string or not'''\n",
    "    if not deep_clean:\n",
    "        return text.upper()\n",
    "    else:\n",
    "        return text.translate(str.maketrans('','',string.punctuation)).upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Need to remove infrequent words for unigram and bigram seperately, need to add codes for this__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_X = process(dev_text, deep_clean=True, stop_words=stopwords, add_bigram=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = process(heldout_text, deep_clean=True, stop_words=stopwords, add_bigram=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(train_path, 'rb') as f:\n",
    "#     dev_X = pickle.load(f)\n",
    "\n",
    "with open(dev_label_path, 'r', encoding='utf-8') as f:\n",
    "    dev_y = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(X, y, percentage=0.75):\n",
    "    indices = np.random.permutation(len(X))\n",
    "    threshold = int(len(dev_X)*percentage)\n",
    "    training_idx, test_idx = indices[:threshold], indices[threshold:]\n",
    "    return X[training_idx], y[training_idx], X[test_idx], y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, valid_X, valid_y = split_train_test(np.array(dev_X), np.array(dev_y), 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect vocabulary\n",
    "vocabulary = set(np.r_['0', np.concatenate(train_X),np.concatenate(valid_X),np.concatenate(test_X)])\n",
    "vocabulary_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407059"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_counter = Counter(np.r_['0', np.concatenate(train_X),np.concatenate(valid_X),np.concatenate(test_X)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replace_list(vocabulary_counter, top_threshold_num, low_threshold_frequency):\n",
    "    '''get a list of removal words that is < top_thres or > low_thres regarding frequency'''\n",
    "    top_removals = [pair[0] for pair in vocabulary_counter.most_common(top_threshold_num)]\n",
    "    low_removals = [pair[0] for pair in vocabulary_counter.items() if pair[1]<low_threshold_frequency]\n",
    "    return top_removals+low_removals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399044"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unknownwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of replacing words that is on the top 3 in frequency ranking or having less than 2 frequency\n",
    "unknownwords = get_replace_list(vocabulary_counter, 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_X, train_y, vocabulary_size, unknownwords=None):\n",
    "    '''train model based on training set and return counts\n",
    "    '''\n",
    "    train_label_count =  Counter(train_y)\n",
    "\n",
    "    train_label_probs = {label: train_label_count.get(label)/len(train_y) for label in train_label_count}\n",
    "    \n",
    "    train_probs = {}\n",
    "    sudo_probs = {}\n",
    "    \n",
    "    for label in train_label_count.keys():\n",
    "        train_x_label = train_X[train_y == label]\n",
    "        # train counter\n",
    "        train_X_concatenate = np.concatenate(train_x_label)\n",
    "        # replace words that is unknownwords\n",
    "        if unknownwords is not None:\n",
    "            train_X_cleaned = [word if word not in unknownwords else '<UNK>' for word in np.concatenate(train_x_label)]\n",
    "        else:\n",
    "            train_X_cleaned = train_X_concatenate\n",
    "        train_counter = Counter(train_X_cleaned)\n",
    "        train_N = len(np.concatenate(train_X))\n",
    "\n",
    "        tokens = list(train_counter.keys())\n",
    "        counts = np.array(list(train_counter.values()))\n",
    "\n",
    "        # smoothing\n",
    "        probs = (counts+1)/(train_N+vocabulary_size)\n",
    "\n",
    "        train_prob = {token:prob for token, prob in zip(tokens, probs) }\n",
    "\n",
    "        sudo_prob = 1/(train_N+vocabulary_size)\n",
    "        \n",
    "        train_probs.update({label: train_prob})\n",
    "        sudo_probs.update({label: sudo_prob})\n",
    "    \n",
    "    return train_probs, train_label_probs, sudo_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probs, train_label_probs, train_sudo_probs = train_model(train_X, train_y, vocabulary_size, unknownwords=unknownwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(train_probs, train_label_probs, train_sudo_probs, test_X, unknownwords=None):\n",
    "    '''\n",
    "    predict using trained naive bayes model, and return label predictions;\n",
    "    calculation is based on log base'''\n",
    "    prediction_results = []\n",
    "    for X in test_X:\n",
    "        if unknownwords is None:\n",
    "            words = X\n",
    "        else:\n",
    "            words = [word if word not in unknownwords else '<UNK>' for word in X]\n",
    "        log_probs = {}\n",
    "        for label in train_label_probs.keys():\n",
    "            # get corresponding model\n",
    "            label_prob = train_label_probs.get(label)\n",
    "            word_prob = train_probs.get(label)\n",
    "            sudo_prob = train_sudo_probs.get(label)\n",
    "            # predict\n",
    "            log_prob = [np.log(word_prob.get(word)) if word_prob.get(word) is not None else np.log(sudo_prob) for word in words]\n",
    "            log_prob = sum(log_prob)+np.log(label_prob)\n",
    "            log_probs.update({label: log_prob})\n",
    "        prediction = sorted(log_probs.items(), key=lambda x:x[1],reverse=True)[0][0]\n",
    "        prediction_results.append(prediction)\n",
    "    return prediction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_prediction, test_y):\n",
    "    '''evaluate the prediction and return accuracy and prediction and recall for all classes'''\n",
    "    correct_prediction = 0\n",
    "    classes = set(test_y)\n",
    "    # for each class\n",
    "    correct_predicteds = {label:0 for label in classes}\n",
    "    num_of_labels = {label:0 for label in classes}\n",
    "    num_of_predictions = {label:0 for label in classes}\n",
    "    \n",
    "    for prediction, label in zip(test_prediction, test_y):\n",
    "        num_of_predictions.update({prediction:num_of_predictions.get(prediction)+1})\n",
    "        num_of_labels.update({label:num_of_labels.get(label)+1})\n",
    "        if prediction == label:\n",
    "            correct_prediction+=1\n",
    "            correct_predicteds.update({label: correct_predicteds.get(label)+1})\n",
    "    accuracy = correct_prediction/len(test_prediction)\n",
    "    precisions = {label: float(correct_predicteds.get(label))/num_of_predictions.get(label) if num_of_predictions.get(label)!=0 else 0 for label in classes}\n",
    "    recalls = {label: float(correct_predicteds.get(label))/num_of_labels.get(label) if num_of_predictions.get(label)!=0 else 0 for label in classes}\n",
    "    return accuracy, precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y, y_hat):\n",
    "    num_accurate = 0\n",
    "    for true_value, predicted_value in zip(y, y_hat):\n",
    "        if true_value == predicted_value:\n",
    "            num_accurate+=1\n",
    "    return num_accurate/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(results, output_path):\n",
    "    '''add results to the output file'''\n",
    "    with open(output_path, 'a') as f:\n",
    "        # acc\n",
    "        print('overall accuracy', file = f)\n",
    "        print(results[0], file = f)\n",
    "        print('precision for red', file = f)\n",
    "        print(results[1].get('RED'), file = f)\n",
    "        print('recall for red', file = f)\n",
    "        print(results[2].get('RED'), file = f)\n",
    "        print('precision for blue', file = f)\n",
    "        print(results[1].get('BLUE'), file = f)\n",
    "        print('recall for blue', file = f)\n",
    "        print(results[2].get('BLUE'), file = f)\n",
    "        print(file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_prediction = predict(train_probs, train_label_probs, train_sudo_probs, valid_X, unknownwords=unknownwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(valid_prediction, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.824,\n",
       " {'neg': 0.8861386138613861, 'pos': 0.7818791946308725},\n",
       " {'neg': 0.7336065573770492, 'pos': 0.91015625})"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(valid_prediction, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = predict(train_probs, train_label_probs, train_sudo_probs, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_results = evaluate(test_1_prediction, test_1_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(test_1_results, task_1_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_env]",
   "language": "python",
   "name": "conda-env-nlp_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
